{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labo 7 : classification de textes (pour la désambiguïsation lexicale)\n",
    "Nathan Gonzalez Montes & Guidoux Vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectif et plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif de ce laboratoire est d’utiliser des méthodes d’apprentissage supervisé pour classifier des occurrences du mot interest selon leur sens : c’est la même tâche, avec les mêmes données, que le labo 6. Pour classifier les occurrences, on considère leur contexte (mots voisins), et on applique l’approche bayésienne vue en cours : on entraîne un classifieur à six classes (les six sens de interest annotés de 1 à 6) sur une partie des données et on le teste sur la partie restante.\n",
    "\n",
    "Dans ce laboratoire, on explore deux façons différentes de coder les traits (features) pour cette tâche. Dans les deux cas, on entraînera un NaiveBayesClassifier fourni par NLTK.1 Les deux façons sont :\n",
    "1. Constituer un vocabulaire des mots qui apparaissent dans le voisinage de interest et définir ces mots comme traits. Pour chaque occurrence de interest, on extrait la valeur de ces traits sous la forme `{(‘rate’ : True), (‘in’ : False), … }` et on ajoute la classe (de 1 à 6).\n",
    "2. Si word-1 est le mot précédant l’occurrence de interest, on définit comme traits word-n, …, word-2, word-1, word+1, word+2, …, word+n (une fenêtre de taille 2n autour de interest). Les valeurs possibles de ces traits sont cette fois-ci les mots observés, ou ‘NONE’ si la fenêtre dépasse les limites de la phrase. Pour chaque occurrence de interest, on extrait la valeur de ces traits sous la forme `{(‘word-1’ : ‘his’), (‘word+1’ : ‘in’), … }` et on ajoute la classe (de 1 à 6).\n",
    "\n",
    "Dans les deux cas, il faut diviser les 2368 occurrences de interest en un jeu d’entraînement et un jeu de test, en respectant la proportion initiale de chaque sens. Puis on entraîne un classifieur bayésien naïf en respectant le format de données indiqué par NLTK2, et on teste la performance du classifieur entraîné. L’objectif est de trouver les paramètres qui conduisent aux meilleurs scores de WSD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pylab as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des noms propres, ponctuation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étapes proposées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Traits lexicaux : présence ou absence de mots dans le voisinage de interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Le fichier de données se trouve à http://www.d.umn.edu/~tpederse/data.html – chercher « interest » vers la fin de la page, et prendre le fichier marqué comme « original format without POS tags » (le même qu’au labo 6). Lire le fichier et générer une liste de listes de mots (une liste par phrase) appelée `tokenized_sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ponctuations = [',', '.', '`', '\\'','-', ';', ':', ')', '&', '{', '(', '}', '=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 2368 phrases \n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/interest-original.txt' \n",
    "\n",
    "try:  \n",
    "    fp = open(filepath, 'r',encoding=\"utf-8\")\n",
    "    raw_sentences = fp.read()\n",
    "finally:  \n",
    "    fp.close()\n",
    "    \n",
    "for ponct in ponctuations:\n",
    "    raw_sentences = raw_sentences.replace(ponct, ' ') # clear the data\n",
    "\n",
    "raw_sentences = re.sub(' [0-9]+', '', raw_sentences) # clear the data\n",
    "\n",
    "raw_sentences = raw_sentences.split('\\n$$\\n') # clear the data\n",
    "\n",
    "raw_sentences = raw_sentences[:-1] # la dernière phrase est vide\n",
    "\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in raw_sentences] \n",
    "\n",
    "print(\"Il y a {} phrases \".format(len(tokenized_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Définir une variable `window_size`, par exemple égale à 3 (on la fera varier plus tard), et une liste vide de mots `word_list`. Parcourir les `tokenized_sentences` et pour chaque phrase ajouter les mots voisins de interest (i.e. situés à une distance inférieure ou égale à `window_size`) dans la liste de mots `word_list`. Combien de mots contient celle-ci à la fin ? Tokens ou types ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowation(tokenized_sentences, windows_size):\n",
    "    \n",
    "    word_dict = {1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
    "    vocabulary = set()\n",
    "    word_list = []\n",
    "       \n",
    "    for sentence in (tokenized_sentences):  \n",
    "        i = None\n",
    "        current_sens = None\n",
    "        for def_i in range(1,7):\n",
    "            current_sens = def_i\n",
    "            if \"interest_{}\".format(def_i) in sentence:\n",
    "                i = sentence.index(\"interest_{}\".format(def_i))\n",
    "                break\n",
    "            elif \"interests_{}\".format(def_i) in sentence:\n",
    "                i = sentence.index(\"interests_{}\".format(def_i))\n",
    "                break\n",
    "        if i != None:\n",
    "            for index in range(-windows_size, windows_size+1):\n",
    "                current_index = index + i\n",
    "                if current_index > 0 and current_index < len(sentence) and index != 0:\n",
    "                    current_word = sentence[current_index]\n",
    "                    word_dict[current_sens].append(current_word)\n",
    "                    vocabulary.add(current_word)\n",
    "                    word_list.append(current_word)\n",
    "        else:\n",
    "            print(\"ERROR\")\n",
    "            \n",
    "    return (word_dict, vocabulary, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size = 3\n",
    "\n",
    "word_dict, vocabulary, word_list = windowation(tokenized_sentences, windows_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list 1 : 1954\n",
      "word_list 2 : 65\n",
      "word_list 3 : 309\n",
      "word_list 4 : 935\n",
      "word_list 5 : 2772\n",
      "word_list 6 : 6759\n"
     ]
    }
   ],
   "source": [
    "somme = 0\n",
    "\n",
    "for i in range(1,7):\n",
    "    current_list = word_dict[i]\n",
    "    print(\"word_list {} : {}\".format(i, len(current_list)))\n",
    "    somme += len(current_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre total d'occurances : 12794\n",
      "nombre de mots différents : 2274\n"
     ]
    }
   ],
   "source": [
    "print(\"nombre total d'occurances : {}\".format(somme))\n",
    "print(\"nombre de mots différents : {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. À l’aide d’un objet `NLTK` de type `FreqDist`, sélectionner parmi les mots de `word_list` les `N` plus fréquents, dans une nouvelle liste appelée `vocabulary` (p.ex. `N = 500`, mais on le fera varier). Affichez les 50 mots les plus fréquents. Est-ce une bonne idée d’enlever les stopwords ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "vocabulary = nltk.FreqDist(word_list).most_common(N)\n",
    "vocabulary = np.array(vocabulary)\n",
    "vocabulary = vocabulary[:, 0] # Nous ne gardons que la première colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 770),\n",
       " ('rates', 624),\n",
       " ('the', 608),\n",
       " ('and', 408),\n",
       " ('to', 379),\n",
       " ('of', 333),\n",
       " ('a', 292),\n",
       " ('s', 181),\n",
       " ('on', 172),\n",
       " ('%', 163),\n",
       " ('rate', 144),\n",
       " ('its', 134),\n",
       " ('payments', 112),\n",
       " ('that', 103),\n",
       " ('are', 102),\n",
       " ('for', 93),\n",
       " ('has', 91),\n",
       " ('with', 89),\n",
       " ('lower', 82),\n",
       " ('an', 82),\n",
       " ('short', 80),\n",
       " ('is', 72),\n",
       " ('have', 69),\n",
       " ('by', 68),\n",
       " ('at', 65),\n",
       " ('will', 64),\n",
       " ('high', 63),\n",
       " ('from', 63),\n",
       " ('u', 60),\n",
       " ('term', 52),\n",
       " ('$', 52),\n",
       " ('company', 49),\n",
       " ('foreign', 48),\n",
       " ('annual', 48),\n",
       " ('their', 48),\n",
       " ('which', 47),\n",
       " ('or', 46),\n",
       " ('minority', 46),\n",
       " ('bonds', 44),\n",
       " ('higher', 43),\n",
       " ('said', 43),\n",
       " ('income', 42),\n",
       " ('other', 41),\n",
       " ('as', 41),\n",
       " ('pay', 40),\n",
       " ('it', 39),\n",
       " ('below', 39),\n",
       " ('t', 36),\n",
       " ('be', 35),\n",
       " ('up', 35)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(word_list).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Est-ce une bonne idée d’enlever les stopwords ?**\n",
    "> blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Parcourir à nouveau les `tokenized_sentences` et pour chaque phrase créer un couple (dictionnaire, sens), où le dictionnaire regroupe les traits et leurs valeurs, et le sens est un nombre de 1 à 6 indiquant le sens de interest. Les couples pour toutes les phrases seront rassemblés dans une liste appelée `feature_sets`.\n",
    "- Prendre modèle sur https://www.nltk.org/book/ch06.html (début du 1.2)\n",
    "- Pour le dictionnaire, il faut créer un trait pour chaque mot de vocabulary, et examiner si ce mot est présent dans une fenêtre de taille window_size autour de l’occurrence de interest : si oui, le trait est True, sinon il est False. Par exemple, on aboutit à : {'contains(the)': False, 'contains(,)': True, 'contains(rates)': True, …}.\n",
    "- Ajouter aussi le trait ‘word0’ qui note si l’occurrence est interest ou interests (pluriel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(sentence, windows_size, vocabulary):\n",
    "    feature_set = {}\n",
    "    current_sens = None\n",
    "    word0 = False\n",
    "    for def_i in range(1,7):\n",
    "        current_sens = def_i\n",
    "        current_interest = \"interest_{}\".format(def_i)\n",
    "        current_interests = \"interests_{}\".format(def_i)\n",
    "        if current_interest in sentence:\n",
    "            i = sentence.index(current_interest)\n",
    "            break\n",
    "        elif current_interests in sentence:\n",
    "            i = sentence.index(current_interests)\n",
    "            word0 = True\n",
    "            break\n",
    "    feature_set['word0'] = word0\n",
    "    if i != None:\n",
    "        neighbours = []\n",
    "        for index in range(-windows_size, windows_size+1):\n",
    "            current_index = index + i\n",
    "            if current_index > 0 and current_index < len(sentence) and index != 0:\n",
    "                neighbours.append(sentence[current_index])\n",
    "        for trait in vocabulary:\n",
    "            if trait in neighbours:\n",
    "                feature_set[trait] = True\n",
    "            else:\n",
    "                feature_set[trait] = False\n",
    "            \n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "    return (feature_set,current_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2368"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    feature_sets.append(gender_features(sentence, windows_size, vocabulary))\n",
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Combien d’occurrences pour chaque sens de interest y a-t-il dans feature_sets ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "somme = 0\n",
    "for feature_set in feature_sets:\n",
    "    occurrences[feature_set[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 361, 2: 11, 3: 66, 4: 178, 5: 500, 6: 1252}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Diviser les données de feature_sets en deux sous-ensembles : l’un comportant 80% des données est le `train_set`, et l’autre (20%) est le `test_set`. Attention, il faut respecter deux conditions :\n",
    "- Mélanger avec shuffle() les occurrences avant de prendre les 80% premières pour le `train_set`et les 20% restantes dans le test_set.\n",
    "- Chaque sens doit être présent dans les mêmes proportions dans train_set et dans test_set (donc il faut faire la division de manière séparée pour chaque sens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_test(feature_sets):\n",
    "\n",
    "    random.shuffle(feature_sets)\n",
    "\n",
    "    occurence_in_train = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for feature_set in feature_sets:\n",
    "        current_sens = feature_set[1]\n",
    "        if occurence_in_train[current_sens] < (occurrences[current_sens] * 0.8):\n",
    "            train_set.append(feature_set)\n",
    "            occurence_in_train[current_sens] += 1\n",
    "        else:\n",
    "            test_set.append(feature_set)\n",
    "    return (train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1896 couples dans train et 472 dans test\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = separate_train_test(feature_sets)\n",
    "\n",
    "print(\"Il y a {} couples dans train et {} dans test\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Entraîner un classifieur de type `NaiveBayesClassifier` de `NLTK` sur `train_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**puis le tester sur les données de `test_set`. Quelle est la précision (`accuracy`) atteinte ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision atteinte est de 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"La précision atteinte est de {:.2f}\".format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Adapter le code précédent pour effectuer plusieurs divisions des données en train et test (par exemple 10), et calculer la moyenne des scores obtenus. Comment se compare cette moyenne avec votre premier résultat ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(feature_sets):\n",
    "    train_set, test_set = separate_train_test(feature_sets)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----itération n°0-----\n",
      "-----itération n°1-----\n",
      "-----itération n°2-----\n",
      "-----itération n°3-----\n",
      "-----itération n°4-----\n",
      "-----itération n°5-----\n",
      "-----itération n°6-----\n",
      "-----itération n°7-----\n",
      "-----itération n°8-----\n",
      "-----itération n°9-----\n"
     ]
    }
   ],
   "source": [
    "N_iteration = 10\n",
    "accuracies = []\n",
    "for iteration in range(N_iteration):\n",
    "    print(\"-----itération n°{}-----\".format(iteration))\n",
    "    accuracies.append(train_test(feature_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la moyenne des scores obtenus est de 0.88 \n"
     ]
    }
   ],
   "source": [
    "print(\"la moyenne des scores obtenus est de {:.2f} \".format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Cherchez les meilleurs paramètres pour la taille de la fenêtre (p.ex. 1, 3, 5, 7, 11) et la taille du vocabulaire (50, 100, 200, 500, 1000 mots). Combien d’expériences faut-il exécuter ? Quelle est la meilleure combinaison fenêtre x vocabulaire et quel est le score moyen obtenu ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iteration = 10\n",
    "windows_sizes = [1,3,5,7,11]\n",
    "# windows_sizes = [1,3]\n",
    "vocabulary_sizes =[50, 100, 200, 500, 1000]\n",
    "# vocabulary_sizes =[50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.empty((len(vocabulary_sizes),len(windows_sizes)),dtype=float)\n",
    "np.shape(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ windows_size : 1; vocabulary_size : 50 ------------\n",
      "la moyenne des scores obtenus est de 0.847 en 57 secondes \n",
      "\n",
      "------------ windows_size : 1; vocabulary_size : 100 ------------\n",
      "la moyenne des scores obtenus est de 0.853 en 59 secondes \n",
      "\n",
      "------------ windows_size : 1; vocabulary_size : 200 ------------\n",
      "la moyenne des scores obtenus est de 0.857 en 55 secondes \n",
      "\n",
      "------------ windows_size : 1; vocabulary_size : 500 ------------\n"
     ]
    }
   ],
   "source": [
    "for i, windows_size in enumerate(windows_sizes):\n",
    "    for j, vocabulary_size in enumerate(vocabulary_sizes):\n",
    "        start = time.time()\n",
    "        print(\"\\n------------ windows_size : {}; vocabulary_size : {} ------------\".format(windows_size, vocabulary_size))\n",
    "        \n",
    "        word_dict, vocabulary, word_list = windowation(tokenized_sentences, windows_size)\n",
    "        \n",
    "        somme = 0\n",
    "        \n",
    "        for sens_i in range(1,7):\n",
    "            current_list = word_dict[sens_i]\n",
    "#             print(\"word_list {} : {}\".format(i, len(current_list)))\n",
    "            somme += len(current_list)\n",
    "        \n",
    "#         print(\"nombre total d'occurances : {}\".format(somme))\n",
    "#         print(\"nombre de mots différents : {}\".format(len(vocabulary)))\n",
    "        \n",
    "        vocabulary = nltk.FreqDist(word_list).most_common(N)\n",
    "        vocabulary = np.array(vocabulary)\n",
    "        vocabulary = vocabulary[:, 0]\n",
    "        \n",
    "        feature_sets = []\n",
    "\n",
    "        for sentence in tokenized_sentences:\n",
    "            feature_sets.append(gender_features(sentence, windows_size, vocabulary))\n",
    "        len(feature_sets)\n",
    "        \n",
    "        occurrences = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "        for feature_set in feature_sets:\n",
    "            occurrences[feature_set[1]] += 1\n",
    "        \n",
    "        train_set, test_set = separate_train_test(feature_sets)\n",
    "#         print(\"Il y a {} couples dans train et {} dans test\".format(len(train_set), len(test_set)))\n",
    "        \n",
    "        accuracies = []\n",
    "        for iteration in range(N_iteration):\n",
    "            accuracies.append(train_test(feature_sets))\n",
    "            \n",
    "        current_accuracy = np.mean(accuracies)\n",
    "        \n",
    "        scores[i][j] = current_accuracy\n",
    "        \n",
    "        end = time. time()\n",
    "\n",
    "        execution_time = end-start\n",
    "        print(\"la moyenne des scores obtenus est de {:.3f} en {:.0f} secondes \".format(current_accuracy, execution_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(num=None,figsize=(10,14),dpi=250) \n",
    "heatmap = ax.pcolor(scores, cmap=plt.cm.gray)\n",
    "\n",
    "ax.set_xticks(np.arange(scores.shape[1])+0.5, minor=False)\n",
    "ax.set_yticks(np.arange(scores.shape[0])+0.5, minor=False)\n",
    "\n",
    "ax.set_frame_on(False)\n",
    "ax.xaxis.tick_top()\n",
    "ax.grid(False)\n",
    "plt.xlim([0,np.shape(scores)[1]])\n",
    "\n",
    "ax.set_yticklabels(windows_sizes, minor=False) \n",
    "ax.set_xticklabels(vocabulary_sizes, minor=False)\n",
    "\n",
    "plt.xticks(rotation=90) # rotate xlabels\n",
    "matplotlib.rcParams['xtick.labelsize'] = 10\n",
    "\n",
    "cbar = fig.colorbar(heatmap, orientation='horizontal')\n",
    "cbar.set_label('Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Traits lexicaux positionnels : valeurs des mots précédant/suivant interest\n",
    "Pour cette deuxième partie, on réutilisera beaucoup d’éléments de la première. Seule la nature des\n",
    "traits utilisés et leur extraction vont changer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Partir de la liste de listes de mots (une liste par phrase) précédente, appelée tokenized_sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Définir une variable window_size2, par exemple égale à 3 (on la fera varier plus tard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Parcourir les tokenized_sentences et pour chaque phrase créer un couple (dictionnaire, sens), où le dictionnaire regroupe les traits et leurs valeurs, et le sens est un nombre de 1 à 6 indiquant le sens de interest. Les couples pour toutes les phrases seront rassemblés dans une nouvelle liste appelée feature_sets2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Pour le dictionnaire de traits, il faut cette fois-ci créer un trait pour chaque position relative par rapport à interest, donc ‘word-1’, ‘word+1’, etc. (jusqu’à window_size2). La valeur du trait sera le mot trouvé à cette position, ou ‘NONE’ si on sort de la phrase. Par exemple {(‘word-1’ : ‘his’), (‘word+1’ : ‘in’), … }."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Ajouter aussi le trait ‘word0’ qui note si l’occurrence est interest ou interests (pluriel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Diviser les données de feature_sets2 en deux sous-ensembles (80%/20%) appelés train_set2 et test_set2 avec la même procédure qu’à la partie A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Entraîner un classifieur de type NaiveBayesClassifier de NLTK sur train_set2, puis le tester sur les données de test_set2. Quelle est la précision (accuracy) atteinte ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Effectuer plusieurs divisions des données en train et test (par exemple 10), et calculer la moyenne des scores obtenus. Comment se compare cette moyenne avec votre premier résultat ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Cherchez les meilleurs paramètres pour la taille de la fenêtre (p.ex. entre 1 et 15). Quelle est la meilleure valeur et quel est le score moyen obtenu ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Quelle est le meilleur score obtenu entre (A) et (B) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Thème de réflexion facultatif : les différences des scores sont-elles statistiquement significatives ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merci d’envoyer votre notebook Jupyter par email au professeur avant le **lundi 27 mai à 23h59**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
